{"cmd": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nimport imageio\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport torchvision.utils as vutils\n\n# ================== 配置参数 ==================\nclass Config:\n    batch_size = 100\n    latent_dim = 20\n    epochs = 10\n    num_classes = 10\n    img_dim = 28\n    initial_filters = 16\n    intermediate_dim = 256\n    lamb = 2.5  # 重构损失权重\n    sample_std = 0.5  # 采样标准差\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ================== 数据加载 ==================\ndef get_dataloaders():\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x.view(1, Config.img_dim, Config.img_dim))\n    ])\n    \n    train_set = datasets.MNIST('~/Documents/data', train=True, download=True, transform=transform)\n    test_set = datasets.MNIST('~/Documents/data', train=False, transform=transform)\n    \n    train_loader = DataLoader(train_set, batch_size=Config.batch_size, shuffle=True)\n    test_loader = DataLoader(test_set, batch_size=Config.batch_size)\n    \n    return train_loader, test_loader, train_set, test_set\n\n# ================== 模型定义 ==================\nclass GaussianLayer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mean = nn.Parameter(torch.zeros(Config.num_classes, Config.latent_dim))\n        \n    def forward(self, z):\n        # return z.unsqueeze(1) - self.mean.unsqueeze(0)\n        return z.unsqueeze(1) \n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(32, 32, 3, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 64, 3, padding=1),\n            nn.LeakyReLU(0.2)\n        )\n        self.flatten = nn.Flatten()\n        self.fc_mean = nn.Linear(64*7*7, Config.latent_dim)\n        self.fc_logvar = nn.Linear(64*7*7, Config.latent_dim)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.flatten(x)\n        return self.fc_mean(x), self.fc_logvar(x)\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(Config.latent_dim, 64*7*7)\n        self.conv = nn.Sequential(\n            nn.ConvTranspose2d(64, 64, 3, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n            nn.LeakyReLU(0.2),\n            nn.ConvTranspose2d(32, 32, 3, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, z):\n        z = self.fc(z)\n        z = z.view(-1, 64, 7, 7)\n        return self.conv(z)\n\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(Config.latent_dim, Config.intermediate_dim),\n            nn.ReLU(),\n            nn.Linear(Config.intermediate_dim, Config.num_classes),\n            nn.Softmax(dim=1)\n        )\n        \n    def forward(self, z):\n        return self.net(z)\n\nclass ClusterVAE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n        self.classifier = Classifier()\n        self.gaussian = GaussianLayer()\n        \n    def reparameterize(self, mean, logvar):\n        std = torch.exp(0.5*logvar)\n        eps = torch.randn_like(std)\n        return mean + eps*std\n        \n    def forward(self, x):\n        z_mean, z_logvar = self.encoder(x)\n        z = self.reparameterize(z_mean, z_logvar)\n        return {\n            'recon': self.decoder(z),\n            'z_prior': self.gaussian(z),\n            'y': self.classifier(z), # p(y|x)\n            'z_mean': z_mean,\n            'z_logvar': z_logvar\n        }\n\n# ================== 损失计算 ==================\n# def compute_loss(output, x):\n#     # 重构损失\n#     recon_loss = 0.5 * F.mse_loss(output['recon'], x, reduction='sum') / x.size(0)\n#\n#     # KL散度\n#     z_logvar = output['z_logvar'].unsqueeze(1)\n#     kl_elements = -0.5 * (z_logvar - output['z_prior'].pow(2))\n#\n#     # 方法1:\n#     kl_loss = torch.einsum('bi,bil->b', output['y'], kl_elements).mean()\n#\n#     # 方法2: 修正后的替代方案\n#     # weighted_kl = output['y'].unsqueeze(-1) * kl_elements  # (batch, num_classes, latent_dim)\n#     # kl_loss = weighted_kl.sum(dim=(1,2)).mean()  # 标量\n#\n#     # 分类熵\n#     cat_loss = -(output['y'] * torch.log(output['y'] + 1e-8)).sum(1).mean()\n#     print(f'y: {output[\"y\"].argmax(dim=1)}')\n#\n#     return Config.lamb * recon_loss + kl_loss + cat_loss\n\ndef compute_loss(output, x, labels):\n    # 重构损失: MSE loss averaged over the batch\n    recon_loss = 0.5 * F.mse_loss(output['recon'], x, reduction='sum') / x.size(0)\n    \n    # KL散度\n    # Expand z_logvar to have a new dimension for the num_classes (if needed)\n    z_logvar = output['z_logvar'].unsqueeze(1)  # shape: (batch, 1, latent_dim)\n    # Here, we assume output['z_prior'] is of shape (batch, num_classes, latent_dim)\n    # and compute elementwise: -0.5 * (z_logvar - (z_prior)^2)\n    kl_elements = -0.5 * (z_logvar - output['z_prior'].pow(2))\n    \n    # Using einsum to mimic K.batch_dot(K.expand_dims(y,1), kl_elements)\n    kl_loss = torch.einsum('bi,bil->b', output['y'], kl_elements).mean()\n    \n    # 分类熵（原始正则项）\n    cat_loss = -(output['y'] * torch.log(output['y'] + 1e-8)).sum(1).mean()\n    \n    # 新增：分类损失：交叉熵损失 (由于 classifier 已经输出概率, 使用 nll_loss 需取对数)\n    classification_loss = F.nll_loss(torch.log(output['y'] + 1e-8), labels)\n    total_loss =  recon_loss + kl_loss + cat_loss \n    total_loss *=  classification_loss\n    print(f'y: {output[\"y\"].argmax(dim=1)}')\n\n    return total_loss\n\n# ================== 训练流程 ==================\ndef train_model(model, train_loader):\n    model.train()\n    optimizer = optim.Adam(model.parameters())\n    \n    for epoch in range(Config.epochs):\n        total_loss = 0\n        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{Config.epochs}')\n        for data, labels in pbar:\n            data = data.to(Config.device)\n            labels = labels.to(Config.device)\n            optimizer.zero_grad()\n            \n            output = model(data)\n            # loss = compute_loss(output, data)\n            loss = compute_loss(output, data,labels)\n            print(f'Loss: {loss.item()}')\n            if output['y'].argmax(dim=1).float().mean() == output['y'].argmax(dim=1)[0]:\n                print(f'Loss: {loss.item()}')\n                break\n            \n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            pbar.set_postfix(loss=total_loss/(pbar.n+1))\n            \n        print(f'Epoch {epoch+1} Average Loss: {total_loss/len(train_loader):.4f}')\n\n# ================== 评估函数 ==================\ndef cluster_accuracy(y_true, y_pred):\n    conf_matrix = np.zeros((Config.num_classes, Config.num_classes), dtype=np.int64)\n    for t, p in zip(y_true, y_pred):\n        conf_matrix[t, p] += 1\n    row_ind, col_ind = linear_sum_assignment(-conf_matrix)\n    return conf_matrix[row_ind, col_ind].sum() / len(y_true)\n\ndef evaluate(model, dataloader):\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for data, labels in tqdm(dataloader, desc='Evaluating'):\n            data = data.to(Config.device)\n            output = model(data)\n            preds = output['y'].argmax(dim=1)\n            y_pred.extend(preds.cpu().numpy())\n            y_true.extend(labels.numpy())\n    return cluster_accuracy(y_true, y_pred)\n\n# ================== 样本生成 ==================\ndef cluster_sample(model, dataset, path, category=0, n=8):\n    \"\"\"Display real samples that are clustered into the specified category.\"\"\"\n    model.eval()\n    indices = []\n    \n    loader = DataLoader(dataset, batch_size=512)\n    with torch.no_grad():\n        for data, _ in tqdm(loader, desc='Collecting indices'):\n            data = data.to(Config.device)\n            output = model(data)\n            preds = output['y'].argmax(dim=1)\n            batch_indices = torch.where(preds == category)[0].cpu().numpy()\n            indices.extend(batch_indices)\n    \n    if len(indices) == 0:\n        print(f\"No samples found for category {category}. Skipping sample generation.\")\n        return\n    \n    figure = np.zeros((Config.img_dim*n, Config.img_dim*n))\n    indices = np.random.choice(indices, n*n, replace=len(indices) < n*n)\n    \n    for i in range(n):\n        for j in range(n):\n            idx = indices[i*n + j] if i*n + j < len(indices) else 0\n            digit = dataset[idx][0].squeeze().numpy()\n            figure[i*Config.img_dim:(i+1)*Config.img_dim,\n                   j*Config.img_dim:(j+1)*Config.img_dim] = digit\n    \n    imageio.imwrite(path, (figure * 255).astype(np.uint8))\n\ndef random_sample(model, path, category=0, n=8):\n    \"\"\"根据指定类别生成新样本\"\"\"\n    model.eval()\n    figure = np.zeros((Config.img_dim*n, Config.img_dim*n))\n    \n    with torch.no_grad():\n        mean = model.gaussian.mean[category].to(Config.device)\n        z = mean + torch.randn(n*n, Config.latent_dim).to(Config.device) * Config.sample_std\n        samples = model.decoder(z).cpu().squeeze().numpy()\n    \n    for i in range(n):\n        for j in range(n):\n            figure[i*Config.img_dim:(i+1)*Config.img_dim,\n                   j*Config.img_dim:(j+1)*Config.img_dim] = samples[i*n + j]\n    \n    imageio.imwrite(path, (figure * 255).astype(np.uint8))\n\ndef plot_dataloader_samples(dataloader, path, n=8):\n    \"\"\"Plot samples from dataloader\"\"\"\n    real_batch = next(iter(dataloader))\n    plt.figure(figsize=(8, 8))\n    plt.axis(\"off\")\n    plt.title(\"Training Images\")\n    grid_img = vutils.make_grid(real_batch[0][:n*n], padding=2, normalize=True)\n    plt.imshow(grid_img.permute(1, 2, 0))\n    plt.savefig(path)\n    plt.close()\n\n\n# ================== 主程序 ==================\nif __name__ == \"__main__\":\n    # 初始化\n    train_loader, test_loader, train_set, test_set = get_dataloaders()\n    model = ClusterVAE().to(Config.device)\n    \n    # 训练\n    train_model(model, train_loader)\n    \n    # # 评估\n    train_acc = evaluate(model, train_loader)\n    test_acc = evaluate(model, test_loader)\n    print(f'Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')\n    \n\n    # plot_dataloader_samples(train_loader, 'samples/train_samples.png')\n    #\n    # for c in range(Config.num_classes):\n    #     cluster_sample(model, train_set, f'samples/cluster_{c}.png', c)\n    #     random_sample(model, f'samples/random_{c}.png', c)\n    #\n    # # 生成样本\n    # if not os.path.exists('samples'):\n    #     os.makedirs('samples')\n    #\n    # for c in range(Config.num_classes):\n    #     cluster_sample(model, train_set, f'samples/cluster_{c}.png', c)\n    #     random_sample(model, f'samples/random_{c}.png', c)", "cmd_opts": " --cell_id=NONE -s", "import_complete": 1, "terminal": "tmux"}